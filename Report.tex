%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassInstructor\ \hmwkClassTime): \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Programming Assignment} % Assignment title
\newcommand{\hmwkClass}{CS\ 6370} % Course/class
\newcommand{\hmwkClassInstructor}{Dr. Sutanu Chakraborty} % Teacher/lecturer
\newcommand{\hmwkAuthorNameA}{Yogesh B , EE12B066 } % Your name
\newcommand{\hmwkAuthorNameB}{Sagar J P , CS12B039 } % Your name
\newcommand{\hmwkAuthorNameC}{R Santhosh Kumar , EE12B101 } % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
\vspace{1in}
}

\author{\textbf{\hmwkAuthorNameA} \\ \textbf{\hmwkAuthorNameB} \\ \textbf{\hmwkAuthorNameC}}
\date{03/10/2015} % Insert date here if you want it to appear below your name


\begin{document}


\maketitle
\pagebreak


%Note: Implementation was done on an old machine with 2GB RAM, Dual Core processor. So, all algorithms were slower.
\section{Word spell check:}
\par In word spell check, we are given a single word which is not present in the dictionary and corrections are to be suggested for it.
\par We used Damerau Levenshtein distance as a metric to find the distance between 2 words. It accounts for insertion, deletion, or substitution of a single character, or a transposition of two adjacent characters. Computing this distance for a given word with respect to every word in the dictionary is computationally expensive. So we adopt alternative approaches.\\
\par We initially tried to reduce the search space using n-grams approach. We build a inverted index offline on this dictionary using the bigrams or trigrams. This reduces the search space. However, there is a major problem with this. Suppose a query word is of length 5 with the errors at 2nd and 4th position, we will never have to correct word in our search space. 
Ex: Actual word - HELLO, Query word - HWLKO\\
\par With the inverted index built using  bigrams or  trigrams we are never going to get a word hello. So this method does not work for such cases. To overcome this problem, we used the trie data structure which is built offline on the dictionary. It works only with distances that obey triangle inequality. But Damerau–Levenshtein distance does not obey triangle inequality.\\
\par So we used Levenshtein distance in the trie data structure which only accounts for insertion, deletion, or substitution of a single character. So in this case substitution will cost 2 instead of 1. So when retrieving the words from trie we added a tolerance level to the actual distance within which we want the words. This tolerance level will take care of the extra cost when the transposition will be treated as 2 substitutions.\\
\par Now on this reduced set of words we run the actual Damerau–Levenshtein distance to find the edit distance of the word from the query. To order the words in a rank order we used the noisy model approach. Our assumption is that a mistake in the query is independent of another mistake. So now we multiplied the probability of all mistakes to get the final probability of that word being the correction.\\
\par We obtained the probabilities from the dataset that is present in the norvig website. We built up the probabilities according to the formulas given below. Matrices are formed for each of the operation.

\begin{itemize}
\item del[x,y] is the number of times characters xy were typed as x in the training set
\item add[x,y] is the number of times character x was typed as xy in the training set
\item sub[x,y] is the number of times y was typed as x in the training set
\item rev[x,y] is the number of times xy was typed as yx
\item chars[x,y] is the number of times xy appeared in the training set
\item chars[x] is the number of times x appeared in the training set
\end{itemize}

The likelihood is obtained as follows:
\[ Pr(t|c) = 
\left\{
	\begin{array}{ll}
		\frac{del[c_{p-1},c_{p}]}{chars[c_{p-1},c_{p}]}  & \mbox{if } deletion \\
		\frac{add[c_{p-1},t_{p}]}{chars[c_{p-1}]} & \mbox{if } addition \\
		\frac{sub[t_{p},c_{p}]}{chars[c_{p}]} & \mbox{if } substitution \\
		\frac{rev[c_{p},c_{p+1}]}{chars[c_{p},c_{p+1}]} & \mbox{if } reversal \\
	\end{array}
\right.
\] 
where $c_{p}$ is the $p^{th}$ character of c and $t_{p}$ is the $p^{th}$ character of $t_{p}$. The matrices are obtained for the del, add, sub, rev operations by using the list of correct words and the possible errors for that word. The $chars[x,y]$ is obtained by using the bigram frequencies given. $chars[x]$ is obtained from the word counts itself.  

Once we have got probabilities for all the words we select the top 10 words from them and output them.

\section{Context based Spelling Correction}
Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. These errors occur due to a variety of reasons like homonym confusion, typos or usage errors. To accomplish this task, a list of confusion words are first identified. A confusion set ${w_{1},...,w_{n}}$ implies that each word $w_{i}$ in the set is ambiguous with every other word in the set and our task is to choose the best word given the context. 

Two standard methods have shown to be effective in case of context based spelling correction - context words and collocations. The context words approach looks at the neighbouring words to correct a specific target word, whereas collocations look at local syntax. Both these methods have complementary advantages and effectively combining the methods would boost the performance. [1] proposes two methods to combine these - decision lists and bayesian classifier. Decision list looks at the best discriminative feature to classify a target word, whereas bayesian classifier looks at all features.

\section{Context words}

In the method of context word, given a target word that needs to be disambiguated, we look at the neighbouring words to infer about this word. For instance to disambiguate "desert" and "dessert", we can say that if the neighbours contain words like "arid", it's most probably desert, whereas if the neighbours contain words like "sweet", it is mostly "dessert". In our case, we looked at $+- k$ window around a target word for the task of disambiguation. k was chosen to be $3$.
\\

Let $w_{i}$ be the target word to be disambiguated and $c_{j}$ be the elements in the set of context words corresponding to the target word $w_{i}$. Then, by Baye's rule,
\[
P(w_{i}|c_{-k}....c_{k}) = P(c_{-k}....c_{k}|w_{i})P(w_{i})
\]
To simplify our task, we can approximate that the context are independent given the target word.
\[
P(c_{-k}....c_{k}|w_{i}) = P(c_{-k}|w_{i}).....P(c_{k}|w_{i})
\]
We need to estimate the probabilities $P(c_{j}|w_{i})$ which is done in the training phase. While training, we look at the whole corpus and count the total number of occurrence of the word $w_{i}$, which we denote as $M$. For each occurrence of $w_{i}$, we collect the list of words in the $\pm k$ window and also get their overall count. Let us denote each unique context word to be $c_{i}$ and their corresponding counts to be $m_{i}$. Then,
\[
P(c_{j}|w_{i}) = \frac{m_{j}}{M}
\]
We shall discard those words whose occurrences are either minimum in the corpus (i.e.) we discard those words obeying,
\[
\sum_{1\leq i \leq n} m_{i} < T_{min} 
\] 

and we also discard those words whose occurrences are very high leading to less discriminative power.
\[
\sum_{1\leq i \leq n} m_{i} > M - T_{min} 
\] 

$T_{min}$ was selected to be $10$. Apart from these, it also becomes essential that we remove all those words which are not discriminating. For example, the word 'the' might occur in the context of almost every word in a confusion set and so it wont help us in disambiguation. To accomplish this task, we look at the entropy of occurrence of a context word given target words , i.e., we compute $P(c|w_{i})$ for all $w_{i} \in \mbox{confusion set}$ and take the entropy of these probabilities. Higher the entropy, better is the discriminative power of a context words.

During testing, we parse through the phrase to identify if the word belongs to any of the confusion sets. If so, we pick all the context words in the k window around the target word and estimate the probability $P(w_{i}|c_{-k}....c_{k})$ for all i in the confusion set as mentioned above. Then we pick the maximum probability among all the confusion words and identify it to be the correct word. If a context word doesn't occur around the target word in the training phase, we would get the probability as $0$, which is not desirable. So, we add smoothing to prevent this situation. That is
\[
 P(c_{j}|w_{i}) = \frac{m_{j}+0.5}{M+0.5n} 
 \]
k was selected to be $3$. 
\section{Collocations}

In this method, instead of looking at the context words, we look at the local syntax (i.e.) the pattern of syntactic elements around the target word. For this task, we look at the parts of speech tags of the sequence of $\pm k$ window words around the target word. Unlike the previous case of context words, we can't treat each of the parts of speech tags to be independent and consider a bag of words representation. The sequence becomes very important for collocations. 

Apart from the sequence preservation, inference is similar to the context words. Let us assume that $s_{i}$ be the sequence of parts of speech tags around the target word $w_{i}$. Then 
\[
P(w_{i}|s_{i}) = P(s_{i}|w_{i})P(w_{i})
\]

As before, the priors and likelihoods are estimated in the training phase by collecting the total occurrences of a target word and also the sequences that occur with it. Infrequent sequences and non-discriminative sequences are removed and the residual sequences are used for disambiguation. While testing, the text is parsed to first identify if it belongs to one of the confusion sets. The sequence around the confusion set (s) is extracted, $P(w_{i}|s)$ is computed for each $w_{i}$ in the confusion set, and the maximum is chosen. Smoothing is performed so as to take care of newly encountered sequence around the target word. k was selected to be $3$.

\section{Bayesian Classifier}

This is a method combining Collocations and Context word features. There exists a complementarity between collocations and context words. Context words pick up generalities that are order independent, and collocations capture order dependent generalities. All the features ($f_{j}$) are listed and sorted in order of decreasing strength. Then, for a particular correction, evidence is gathered from all possible paths taken to achieve that correction, i.e, different word corrections can be followed by the same context and collocation correction. This way, the evidence is gathered for each correction. The confusion word with the highest evidence is selected as the correction ($c_{i}$). 
\[
score(c_{i}) = \frac{\sum_j P(c_{i} | f_{j})P(f_{j})}{\sum_i \sum_j P(c_{i} | f_{j})P(f_{j})}
\]
The corrections are sorted by the scores and the top 3 are suggested. 

\section{Dataset}
In our word check experiments, we used the Natural Language Corpus from "norvig.com". In our phrase and sentence check experiments, we used Brown corpus to extract the data needed for likelihood and prior computation. Confusion sets were gathered as follows - We first used the 18 confusion sets given in [1], then we looked at many online sources for list of commonly confused words. We collected all such lists and pruned them based on the number of occurrences in brown corpus. It turned out that Brown corpus was a small dataset as a result of which many confused sets were knocked out.


\begin{thebibliography}{1}
\bibitem{latexcompanion} 
Andrew R. Golding,
\textit{A Bayesion Hybrid Method for context-sensitive spelling correction}.
\bibitem{latexcompanion} 
Mark D. Kemighan, Kenneth W. Church, William A. Gale,
\textit{A Spelling Correction Program Based on a Noisy Channel Model}.


\end{document}
